---
title: "lstms_for_predictive_maintenance-master"
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---
# This notebook implements a keras LSTM in R in comparison to the python implementation

It is not yet well known that keras is available for R. See details on this topic at https://tensorflow.rstudio.com/keras/ where there is also installation instructions.

It is even possible to use tensorboard from within R, see https://tensorflow.rstudio.com/keras/articles/training_visualization.html and more on tensorboard at https://www.tensorflow.org/get_started/summaries_and_tensorboard. The video "Hands-on TensorBoard (TensorFlow Dev Summit 2017)"  https://www.youtube.com/watch?v=eBbEDRsCmv4 is a great starting point

The original implementation in python is located at https://github.com/Azure/lstms_for_predictive_maintenance 


## Deep Learning for Predictive Maintenance 
Deep learning has proven to show superior performance in certain domains such as object recognition and image classification. It has also gained popularity in domains such as finance where time-series data plays an important role. Predictive Maintenance is also a domain where data is collected over time to monitor the state of an asset with the goal of finding patterns to predict failures which can also benefit from certain deep learning algorithms. Among the deep learning methods, Long Short Term Memory (LSTM) networks are especially appealing to the predictive maintenance domain due to the fact that they are very good at learning from sequences. This fact lends itself to their applications using time series data by making it possible to look back for longer periods of time to detect failure patterns. In this notebook, we build an LSTM network for the data set and scenerio described at Predictive Maintenance Template to predict remaining useful life of aircraft engines. In summary, the template uses simulated aircraft sensor values to predict when an aircraft engine will fail in the future so that maintenance can be planned in advance.
This notebook serves as a tutorial for beginners looking to apply deep learning in predictive maintenance domain and uses a simple scenario where only one data source (sensor values) is used to make predictions. In more advanced predictive maintenance scenarios such as in Predictive Maintenance Modelling Guide, there are many other data sources (i.e. historical maintenance records, error logs, machine and operator features etc.) which may require different types of treatments to be used in the deep learning networks. Since predictive maintenance is not a typical domain for deep learning, its application is an open area of research.
This notebook uses keras deep learning library with tensorflow as backend.

```{python}
# set python inactive
if 0:
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  import os
  import keras
```


```{r load libraries, message=FALSE, warning=FALSE}
library(tidyverse)
# devtools::install_github("rstudio/keras")
library(keras)
# install_keras()
library(caret) # preprocessing data, calc precision and recall
```


#Data Ingestion
In the following section, we ingest the training, test and ground truth datasets from azure storage. The training data consists of multiple multivariate time series with "cycle" as the time unit, together with 21 sensor readings for each cycle. Each time series can be assumed as being generated from a different engine of the same type. The testing data has the same data schema as the training data. The only difference is that the data does not indicate when the failure occurs. Finally, the ground truth data provides the number of remaining working cycles for the engines in the testing data. You can find more details about the type of data used for this notebook at Predictive Maintenance Template.

```{python}
# Data ingestion - reading the datasets from Azure blob 
if 0:
  os.system("wget http://azuremlsamples.azureml.net/templatedata/PM_train.txt ")
  os.system("wget http://azuremlsamples.azureml.net/templatedata/PM_test.txt")
  os.system("wget http://azuremlsamples.azureml.net/templatedata/PM_truth.txt")
# Data ingestion - reading the datasets from Azure blob 

```




```{r load data, message=FALSE, warning=FALSE}
# Data ingestion - reading the datasets from Azure blob, has only to run once 
#system("wget http://azuremlsamples.azureml.net/templatedata/PM_train.txt ")
#system("wget http://azuremlsamples.azureml.net/templatedata/PM_test.txt")
#system("http://azuremlsamples.azureml.net/templatedata/PM_truth.txt ")

```

# read training data 
```{python}
if 0:
  train_df = pd.read_csv('PM_train.txt', sep=" ", header=None)
  train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True) # delete columns 26,27
  train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',
                       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',
                       's15', 's16', 's17', 's18', 's19', 's20', 's21'] # set names
```


```{r read training data , message=FALSE, warning=FALSE}
train_df <- (read_delim('PM_train.txt', col_names = FALSE, delim = " ")) %>% select(-c(26+1,27+1)) # plus one because in python columns start at 0
names(train_df) <- c('id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',
                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',
                     's15', 's16', 's17', 's18', 's19', 's20', 's21')
```


# read test data
```{python}
if 0:
  test_df = pd.read_csv('PM_test.txt', sep=" ", header=None)
  test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)
  test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',
                       's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',
                       's15', 's16', 's17', 's18', 's19', 's20', 's21']
```


```{r read test data , message=FALSE, warning=FALSE}
test_df <- (read_delim('PM_test.txt', col_names = FALSE, delim = " ")) %>% select(-c(26+1,27+1))  # plus one because in python columns start at 0
names(test_df) <- c('id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',
                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',
                     's15', 's16', 's17', 's18', 's19', 's20', 's21')
```




# read ground truth data
```{python}
if 0:
  truth_df = pd.read_csv('PM_truth.txt', sep=" ", header=None)
  truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)
```



```{r load ground truth data, message=FALSE, warning=FALSE}
truth_df <- (read_delim('PM_truth.txt', col_names = FALSE, delim = " ")) %>% select(-2)
```

```{python}
if 0:
  train_df = train_df.sort_values(['id','cycle'])
  train_df.head()
```



```{r glimse data, message=FALSE, warning=FALSE}
train_df %>% arrange(id,cycle) -> train_df 
head(train_df)
```

#Data Preprocessing
First step is to generate labels for the training data which are Remaining Useful Life (RUL), label1 and label2 as was done in the Predictive Maintenance Template. Here, we will only make use of "label1" for binary clasification, while trying to answer the question: is a specific engine going to fail within w1 cycles?

# Data Labeling - generate column RUL
```{python}
if 0:
  rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()
  rul.columns = ['id', 'max']
  train_df = train_df.merge(rul, on=['id'], how='left')
  train_df['RUL'] = train_df['max'] - train_df['cycle']
  train_df.drop('max', axis=1, inplace=True)
  train_df.head()
```



```{r trainings Data Labeling, message=FALSE, warning=FALSE}
train_df %>% group_by(id) %>% mutate( RUL = max(cycle) - cycle) -> train_df
head(train_df)
```

# generate label columns for training data
```{python}
if 0:
  w1 = 30
  w0 = 15
  train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )
  train_df['label2'] = train_df['label1']
  train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2
  train_df.head()
```



```{r generate label columns for training data, message=FALSE, warning=FALSE}
w1 = 30
w0 = 15
train_df %>% mutate( label1 = if_else(RUL<= w1, 1,0),
                     label2 = label1,
                     label2 = if_else(RUL<= w0, 2,label2)) -> train_df
```


## normalize data
In the Predictive Maintenance Template , cycle column is also used for training so we will also include the cycle column. Here, we normalize the columns in the training data.

```{python}
if 0:
  # MinMax normalization
  train_df['cycle_norm'] = train_df['cycle']
  cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])
  min_max_scaler = preprocessing.MinMaxScaler()
  norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), 
                               columns=cols_normalize, 
                               index=train_df.index)
  join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)
  train_df = join_df.reindex(columns = train_df.columns)
  train_df.head()
```


```{r MinMax normalization training of training data, message=FALSE, warning=FALSE}

# delete all columns where there is no variance, improves NN performance
noVarCol <- nearZeroVar(train_df)
train_df %>% select(-noVarCol) -> train_df
namesOfCol <- names(train_df)[2:18]
min_max_scaler = preProcess(train_df, verbose = TRUE, method = "range")#
predict(min_max_scaler,train_df) -> train_df_norm
head(train_df_norm)

```





Next, we use the ground truth dataset to generate labels for the test data.
```{python}
if 0:
  # generate column max for test data
  rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()
  rul.columns = ['id', 'max']
  truth_df.columns = ['more']
  truth_df['id'] = truth_df.index + 1
  truth_df['max'] = rul['max'] + truth_df['more']
  truth_df.drop('more', axis=1, inplace=True)
```


```{r test data indexing, message=FALSE, warning=FALSE}
truth_df %>% mutate(id =seq_len(nrow(.))) %>% setNames(c("maxCycle", "id")) -> truth_df

```


# generate RUL for test data
```{python}
if 0:
  test_df = test_df.merge(truth_df, on=['id'], how='left')
  test_df['RUL'] = test_df['max'] - test_df['cycle']
  test_df.drop('max', axis=1, inplace=True)
  test_df.head()
```


```{r test data Labeling, message=FALSE, warning=FALSE}

truth_df %>% mutate(id =seq_len(nrow(.))) %>% setNames(c("maxCycle", "id")) -> truth_df
left_join(test_df,truth_df) -> test_df
test_df %>% group_by(id) %>% mutate( RUL = maxCycle - cycle,
                                     label1 = if_else(RUL<= w1, 1,0),
                                     label2 = label1,
                                     label2 = if_else(RUL<= w0, 2,label2)) %>% select(-maxCycle) -> test_df
predict(min_max_scaler,test_df) -> test_df
head(test_df)

```

```{python}
if 0:
  test_df = test_df.merge(truth_df, on=['id'], how='left')
  test_df['RUL'] = test_df['max'] - test_df['cycle']
  test_df.drop('max', axis=1, inplace=True)
  test_df.head()
```



# generate label columns w0 and w1 for test data
```{python}
if 0:
  test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )
  test_df['label2'] = test_df['label1']
  test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2
  test_df.head()
```


## note in the original flow this section was before 
Next, we prepare the test data. We first normalize the test data using the parameters from the MinMax normalization applied on the training data.
```{python}
if 0:
  test_df['cycle_norm'] = test_df['cycle']
  norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), 
                              columns=cols_normalize, 
                              index=test_df.index)
  test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)
  test_df = test_join_df.reindex(columns = test_df.columns)
  test_df = test_df.reset_index(drop=True)
  test_df.head()
```



In the rest of the notebook, we train an LSTM network that we will compare to the results in Predictive Maintenance Template Step 2B of 3 where a series of machine learning models are used to train and evaluate the binary classification model that uses column "label1" as the label.
# Modelling
The traditional predictive maintenance machine learning models are based on feature engineering which is manual construction of right features using domain expertise and similar methods. This usually makes these models hard to reuse since feature engineering is specific to the problem scenario and the available data which varies from one business to the other. Perhaps the most attractive part of applying deep learning in the predictive maintenance domain is the fact that these networks can automatically extract the right features from the data, eliminating the need for manual feature engineering.
When using LSTMs in the time-series domain, one important parameter to pick is the sequence length which is the window for LSTMs to look back. This may be viewed as similar to picking window_size = 5 cycles for calculating the rolling features in the Predictive Maintenance Template which are rolling mean and rolling standard deviation for 21 sensor values. The idea of using LSTMs is to let the model extract abstract features out of the sequence of sensor values in the window rather than engineering those manually. The expectation is that if there is a pattern in these sensor values within the window prior to failure, the pattern should be encoded by the LSTM.
One critical advantage of LSTMs is their ability to remember from long-term sequences (window sizes) which is hard to achieve by traditional feature engineering. For example, computing rolling averages over a window size of 50 cycles may lead to loss of information due to smoothing and abstracting of values over such a long period, instead, using all 50 values as input may provide better results. While feature engineering over large window sizes may not make sense, LSTMs are able to use larger window sizes and use all the information in the window as input. Below, we illustrate the approach.


# pick a large window size of 50 cycles
```{python}
if 0:
  sequence_length = 50  
```

```{r set seq length, message=FALSE, warning=FALSE}
sequence_length = 50

```
Let's first look at an example of the sensor values 50 cycles prior to the failure for engine id 3. We will be feeding LSTM network this type of data for each time step for each engine id.

# preparing data for visualizations 
# window of 50 cycles prior to a failure point for engine id 3
```{python}
if 0:
  engine_id3 = test_df[test_df['id'] == 3]
  engine_id3_50cycleWindow = engine_id3[engine_id3['RUL'] <= engine_id3['RUL'].min() + 50]
  cols1 = ['s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10']
  engine_id3_50cycleWindow1 = engine_id3_50cycleWindow[cols1]
  cols2 = ['s11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']
  engine_id3_50cycleWindow2 = engine_id3_50cycleWindow[cols2]
```


# plotting sensor data for engine ID 3 prior to a failure point - sensors 1-10 
```{python}
if 0:
  ax1 = engine_id3_50cycleWindow1.plot(subplots=True, sharex=True, figsize=(20,20))

```

```{r plot axis 1, message=FALSE, warning=FALSE}
test_df %>% filter(id >00.2, id < 003, RUL > 0) %>% gather(key = sensor, value = value, -c(id,cycle)) -> test_long

ggplot(test_long %>% filter(sensor %in% paste0("s", 1:10)),
       aes(x = cycle, y = value))  + geom_line() + facet_wrap("sensor", scales = "free_y")

```




# plotting sensor data for engine ID 3 prior to a failure point - sensors 11-21 
```{python}
if 0:
  ax2 = engine_id3_50cycleWindow2.plot(subplots=True, sharex=True, figsize=(20,20))

```

```{r plot axis 2, message=FALSE, warning=FALSE}

ggplot(test_long %>% filter(sensor %in% paste0("s", 11:21)),
       aes(x = cycle, y = value))  + geom_line() + facet_wrap("sensor", scales = "free_y")

```

Keras LSTM layers expect an input in the shape of a numpy array of 3 dimensions (samples, time steps, features) where samples is the number of training sequences, time steps is the look back window or sequence length and features is the number of features of each sequence at each time step.
```{python}
# function to reshape features into (samples, time steps, features) 
def gen_sequence(id_df, seq_length, seq_cols):
    """ Only sequences that meet the window-length are considered, no padding is used. This means for testing
    we need to drop those which are below the window-length. An alternative would be to pad sequences so that
    we can use shorter ones """
    data_array = id_df[seq_cols].values
    num_elements = data_array.shape[0]
    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):
        yield data_array[start:stop, :]
        
        
```
analyse the python function
for i in range(3, 6):
...     print(i)

3
4
5
```{r function to reshape features, message=FALSE, warning=FALSE}

gen_sequence <- function(id_df, seq_length, seq_cols){
  
  data_array <- id_df %>% select(seq_cols) 
  num_elements <- nrow(id_df)
  
}
```



# pick the feature columns 
```{python}
if 0:
  sensor_cols = ['s' + str(i) for i in range(1,22)]
  sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']
  sequence_cols.extend(sensor_cols)
```


```{r pick the feature columns , message=FALSE, warning=FALSE}

sensor_cols <- c(paste0("s",1:21), paste0("setting",1:3), "cycle")
sensor_cols <- namesOfCol

```

# generator for the sequences
```{python}
if 0:
  seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) 
             for id in train_df['id'].unique())
```


```{r generator for the sequences , message=FALSE, warning=FALSE}

gen_sequence <- function(df, sequence_length, seq_cols){
  seq_list <- list(NULL)
  for (i in 1:(nrow(df)-sequence_length)) {
    seq_list[[length(seq_list)+1]]  <-   df[ i:(i+sequence_length-1), ] %>% select(seq_cols) %>% unlist
  #  browser()
  }
  seq_list
}

```

# generate sequences and convert to numpy array

```{python}
if 0:
  seq_array = np.concatenate(list(seq_gen)).astype(np.float32)
  seq_array.shape
```

```{r generate sequences and convert to numpy array, message=FALSE, warning=FALSE}
 train_df_norm %>% ungroup() %>% group_by(id) %>% nest() %>% 
  mutate( seq_array = map(data,gen_sequence, sequence_length= sequence_length, seq_cols=sensor_cols))   -> seq_gen
seq_gen_list <- unlist(seq_gen$seq_array)
length(seq_gen_list)
# the following gives for seq_gen_array[,,1]  
# id cycle  setting1  setting2 setting3     s1        s2        s3        s4    s5    s6       s7
 # <dbl> <dbl>     <dbl>     <dbl>    <dbl>  <dbl>     <dbl>     <dbl>     <dbl> <dbl> <dbl>    <dbl>
#1     0     0 0.4597701 0.1666667      100 518.67 0.1837349 0.4068018 0.3097569 14.62     1 0.726248
# which is train_df_norm[1,], therefore transformation works! 
seq_gen_array <- array(seq_gen_list, dim = c(sequence_length,length(sensor_cols),length(seq_gen_list)/(sequence_length*length(sensor_cols)))) 
#dim(seq_gen_array)
#[1]    50    25 15631 in python the dim is (15631, 50, 25)
dim(seq_gen_array)
# use aperm to permutate dimensions
seq_gen_array_perm <- aperm(seq_gen_array, c(3,1,2))


```

# function to generate labels
```{python}
def gen_labels(id_df, seq_length, label):
    data_array = id_df[label].values
    num_elements = data_array.shape[0]
    return data_array[seq_length:num_elements, :]
```


    
```{r function to generate labels, message=FALSE, warning=FALSE}
gen_labels <- function(df, sequence_length, label){
  #browser()
  df[ (sequence_length+1):(nrow(df)), ] %>% select(label) %>% unlist -> labelList
  labelList
  
}

```

# generate labels
```{python}
if 0:
  label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) 
               for id in train_df['id'].unique()]
  label_array = np.concatenate(label_gen).astype(np.float32)
  label_array.shape
```



```{r generate labels, message=FALSE, warning=FALSE}

train_df_norm %>% ungroup() %>% group_by(id) %>% nest() %>% 
  mutate( label_array = map(data,gen_labels, sequence_length= sequence_length, label="label1"))   -> label_df

label_array <- unlist(label_df$label_array)
summary(label_array)
```
### LSTM Network
Next, we build a deep network. The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. Dropout is also applied after each LSTM layer to control overfitting. Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.


## build the network
```{python}
if 0:
  nb_features = seq_array.shape[2]
  nb_out = label_array.shape[1]
  
  model = Sequential()
  
  model.add(LSTM(
           input_shape=(sequence_length, nb_features),
           units=100,
           return_sequences=True))
  model.add(Dropout(0.2))
  
  model.add(LSTM(
            units=50,
            return_sequences=False))
  model.add(Dropout(0.2))
  
  model.add(Dense(units=nb_out, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
```


```{r build the network, message=FALSE, warning=FALSE}

nb_features = dim(seq_gen_array_perm)[3]
nb_out = 1# label_array.shape[1]

model <- keras_model_sequential() 
model %>% 
  layer_lstm(input_shape=c(sequence_length, nb_features),
         units=100,
         return_sequences=TRUE) %>% 
  layer_dropout(0.2) %>% 
  layer_lstm(
          units=50,
          return_sequences=FALSE) %>% 
  layer_dropout(0.2) %>% 
  layer_dense(units=nb_out, activation='sigmoid')
  
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = "adam",
  metrics = c('accuracy')
)


```


### summary of model
```{python}
if 0:
  print(model.summary())
```


```{r summary of model, message=FALSE, warning=FALSE}
summary(model)
```


_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 50, 100)           50400     
_________________________________________________________________
dropout_1 (Dropout)          (None, 50, 100)           0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 50)                30200     
_________________________________________________________________
dropout_2 (Dropout)          (None, 50)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 80,651
Trainable params: 80,651
Non-trainable params: 0
_________________________________________________________________


## fit the network 
```{python}
if 0:

#  %%time 
  model.fit(seq_array, label_array, epochs=10, batch_size=200, validation_split=0.05, verbose=1,
            callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')])
```

Train on 14849 samples, validate on 782 samples
Epoch 1/10
14849/14849 [==============================] - 30s - loss: 0.0832 - acc: 0.9658 - val_loss: 0.0521 - val_acc: 0.9770
Epoch 2/10
14849/14849 [==============================] - 32s - loss: 0.0666 - acc: 0.9712 - val_loss: 0.0421 - val_acc: 0.9770
Epoch 3/10
14849/14849 [==============================] - 32s - loss: 0.0597 - acc: 0.9748 - val_loss: 0.0360 - val_acc: 0.9898
Epoch 4/10
14849/14849 [==============================] - 32s - loss: 0.0538 - acc: 0.9768 - val_loss: 0.0432 - val_acc: 0.9783
CPU times: user 7min 40s, sys: 2min 29s, total: 10min 9s
Wall time: 2min 8s

# To start tensorboard from with R type "tensorboard("logs")" 
```{r fit  model, message=FALSE, warning=FALSE}


tensorboard("logs") # start tensorboard before fitting, only displays values after first batch
 history <- model %>% fit(
  seq_gen_array_perm, label_array, 
  epochs = 15, batch_size = 200, 
   callbacks = callback_tensorboard("logs/run_c"),
  validation_split = 0.05, verbose=1
)

```


```{r fit  model plot, message=FALSE, warning=FALSE}
plot(history)

```

## training metrics
```{python}
if 0:
  scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)
  print('Accurracy: {}'.format(scores[1]))

```

15600/15631 [============================>.] - ETA: 0sAccurracy: 0.9516985426059833

```{r training metrics, message=FALSE, warning=FALSE}
model %>% evaluate(seq_gen_array_perm, label_array)
```

# make predictions and compute confusion matrix
```{python}
if 0:
  y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)
  y_true = label_array
  print('Confusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels')
  cm = confusion_matrix(y_true, y_pred)
  cm
```


15600/15631 [============================>.] - ETA: 0sConfusion matrix
- x-axis is true labels.
- y-axis is predicted labels
Out[29]:
array([[12522,     9],
       [  746,  2354]])

```{r make predictions and compute confusion matrix, message=FALSE, warning=FALSE}
y_pred = model %>%  predict_classes(seq_gen_array_perm,verbose=1, batch_size=200)
y_true = label_array
cat('Confusion matrix\n- x-axis is true labels.\n- y-axis is predicted labels')
cm = confusionMatrix(y_true, y_pred)
cm
```


# compute precision and recall
```{python}
if 0:
  precision = precision_score(y_true, y_pred)
  recall = recall_score(y_true, y_pred)
  print( 'precision = ', precision, '\n', 'recall = ', recall)
```

precision =  0.996191282268 
 recall =  0.75935483871
 
```{r compute precision and recall, message=FALSE, warning=FALSE}
  precision = precision(cm$table)
  recall = recall(cm$table)
  cat( 'precision = ', precision, '\n', 'recall = ', recall)
``` 
 
 